# cloak_cuda roadmap

The **CUDA cloak** (`cloak/cloak_cuda.c`) is a host runtime that combines the standard Lembeh streaming ABI with the normative **ZCTL/1 kernel backplane** (see `normative/zing_zctl1_kernel_backplane_pack_v1`). The goal is to let the same `_ctl` entrypoint handle capability discovery (`CAPS_LIST`), kernel discovery (`KERNEL_LIST`), and kernel execution (`KERNEL_RUN`) for CUDA targets.

This file captures the current state of the implementation and the remaining steps needed when a CUDA-capable machine is available.

## Build & usage

- Build the object file: `make cloak-cuda`
- Link a compiled Zing program with the cloak:
  ```bash
  cc -Iinclude build/your_prog.c cloak/cloak_cuda.c -o your_prog_cuda
  ```
- The cloak currently streams `_in/_out/_log` via stdio, exactly like `cloak/stdio_cloak.c`.
- `_ctl` messages are parsed with the reference helpers in `normative/zing_zctl1_kernel_backplane_pack_v1/c/zctl1.{h,c}`.
- Two logical capabilities are advertised:
  - `cuda.kernel.list`
  - `cuda.kernel.run`
- `KERNEL_LIST` returns a static catalog (`noop`, `tensor_add`) so that tooling can exercise the protocol before real kernels are registered.
- `KERNEL_RUN` is fully parsed and validated, but the implementation currently returns `ok=0` / `err_code=ZCTL1_ERR_BACKEND` with a message that explains whether CUDA support was compiled in.

## Enabling CUDA on a GPU host

1. Install the CUDA toolkit/driver headers so `<cuda.h>` is available.
2. Rebuild with `-DZCC_ENABLE_CUDA_RUNTIME` in `CFLAGS`/`CPPFLAGS`. You can export `CFLAGS="-DZCC_ENABLE_CUDA_RUNTIME"` (and similarly for `CPPFLAGS`) before invoking `make cloak-cuda`.
3. Flesh out `cuda_backend_init()` and `handle_kernel_run()`:
   - Load the CUDA driver (either directly via `<cuda.h>` or lazily through `dlopen`).
   - Populate `struct cuda_backend` with device/module handles.
   - Replace the stub `respond_kernel_run_stub()` path with real kernel launches that map `zctl1_arg` payloads to CUDA kernel arguments.
   - Update the kernel catalog (`CUDA_KERNELS`) to reflect actual cubin/PTX artifacts generated by your toolchain (the struct already carries `kernel_id`, `sig_hash`, and `flags`).
4. Extend the error reporting so `body.ok` flips to `1` on success and the `err_code`/`err_msg` pair is only used for genuine runtime failures.

## Suggested next steps

| Task | Notes |
|------|-------|
| CUDA driver bootstrap | Create a lightweight loader that calls `cuInit`, selects a device, and caches a `CUcontext`. Guard it behind `ZCC_ENABLE_CUDA_RUNTIME` so non-CUDA builds stay stubbed. |
| Kernel registration | Replace the static `CUDA_KERNELS` array with metadata sourced from a manifest (JSON, Zing tables, etc.). Each entry should include module paths, grid/block defaults, and tensor layout hints. |
| Hopper/Tensor marshaling | Implement helpers that read `zctl1_tensor_desc` records from the guest memory (“hopper”), validate bounds, and map them onto CUDA device buffers (possibly via pinned host memory). |
| Recording + replay | Decide how to record kernel launches (req/resp pairs) for deterministic tests. The `docs/ZCTL1_KERNEL_BACKPLANE.md` laws apply here as well. |
| Integration tests | Once a CUDA host is available, craft golden tests that call `_ctl` with `KERNEL_RUN` frames and compare the response payloads / produced data. |

Keeping the protocol plumbing in place now should make the CUDA bring-up mostly about the driver bindings when we switch to the GPU workstation.
